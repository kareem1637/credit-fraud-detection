{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.448154</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V4       V10       V11       V12       V14       V17  Class\n",
       "0  1.378155  0.090794 -0.551600 -0.617801 -0.311169  0.207971      0\n",
       "1  0.448154 -0.166974  1.612727  1.065235 -0.143772 -0.114805      0\n",
       "2  0.379780  0.207643  0.624501  0.066084 -0.165946  1.109969      0\n",
       "3 -0.863291 -0.054952 -0.226487  0.178228 -0.287924 -0.684093      0\n",
       "4  0.403034  0.753074 -0.822843  0.538196 -1.119670 -0.237033      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/creditcard_balanced.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the PCA-treated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = np.array(x_scaled)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN (conditional GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 01:39:38.450948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_dim, label_dim, output_dim):\n",
    "    noise_input = Input(shape=(noise_dim,))\n",
    "    label_input = Input(shape=(label_dim,))\n",
    "    merged_input = Concatenate()([noise_input, label_input])\n",
    "\n",
    "    x = Dense(128)(merged_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(output_dim, activation=\"tanh\")(x)\n",
    "\n",
    "    model = Model([noise_input, label_input], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_dim, label_dim):\n",
    "    data_input = Input(shape=(input_dim,))\n",
    "    label_input = Input(shape=(label_dim,))\n",
    "    merged_input = Concatenate()([data_input, label_input])\n",
    "\n",
    "    x = Dense(512)(merged_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([data_input, label_input], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100  # Dimension of the noise vector\n",
    "label_dim = 1  # Binary label (0 or 1)\n",
    "output_dim = x_scaled.shape[1]  # Number of PCA components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 01:39:41.984387: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=False)\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(noise_dim, label_dim, output_dim)\n",
    "discriminator = build_discriminator(output_dim, label_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_data, real_labels):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    fake_labels = tf.random.uniform([BATCH_SIZE, 1], minval=0, maxval=2, dtype=tf.int32)\n",
    "    fake_labels = tf.cast(fake_labels, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator([noise, fake_labels], training=True)\n",
    "\n",
    "        real_output = discriminator([real_data, real_labels], training=True)\n",
    "        fake_output = discriminator([generated_data, fake_labels], training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = real_loss + fake_loss\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(\n",
    "            gen_loss, generator.trainable_variables\n",
    "        )\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            disc_loss, discriminator.trainable_variables\n",
    "        )\n",
    "\n",
    "        generator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_generator, generator.trainable_variables)\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 01:40:17.405835: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [474592]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-10-17 01:40:17.406179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [474592]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Gen Loss: 0.9166416525840759, Disc Loss: 1.2155795097351074\n",
      "Epoch 1, Gen Loss: 1.0170763731002808, Disc Loss: 1.2108755111694336\n",
      "Epoch 2, Gen Loss: 0.9701282978057861, Disc Loss: 1.2651163339614868\n",
      "Epoch 3, Gen Loss: 0.853053629398346, Disc Loss: 1.265821099281311\n",
      "Epoch 4, Gen Loss: 0.837211012840271, Disc Loss: 1.2979774475097656\n",
      "Epoch 5, Gen Loss: 0.9124760627746582, Disc Loss: 1.2265633344650269\n",
      "Epoch 6, Gen Loss: 0.8193730711936951, Disc Loss: 1.2691962718963623\n",
      "Epoch 7, Gen Loss: 0.8139760494232178, Disc Loss: 1.333698034286499\n",
      "Epoch 8, Gen Loss: 0.8408277630805969, Disc Loss: 1.2767391204833984\n",
      "Epoch 9, Gen Loss: 0.7733614444732666, Disc Loss: 1.3379660844802856\n",
      "Epoch 10, Gen Loss: 0.8587377667427063, Disc Loss: 1.2894034385681152\n",
      "Epoch 11, Gen Loss: 0.8073910474777222, Disc Loss: 1.3393070697784424\n",
      "Epoch 12, Gen Loss: 0.8257250189781189, Disc Loss: 1.31252121925354\n",
      "Epoch 13, Gen Loss: 0.7807550430297852, Disc Loss: 1.3085529804229736\n",
      "Epoch 14, Gen Loss: 0.8197464346885681, Disc Loss: 1.3317813873291016\n",
      "Epoch 15, Gen Loss: 0.7646693587303162, Disc Loss: 1.3305717706680298\n",
      "Epoch 16, Gen Loss: 0.7353140115737915, Disc Loss: 1.3580400943756104\n",
      "Epoch 17, Gen Loss: 0.7705793976783752, Disc Loss: 1.337693691253662\n",
      "Epoch 18, Gen Loss: 0.7421232461929321, Disc Loss: 1.3764941692352295\n",
      "Epoch 19, Gen Loss: 0.7512952089309692, Disc Loss: 1.3122422695159912\n",
      "Epoch 20, Gen Loss: 0.7575085163116455, Disc Loss: 1.3753442764282227\n",
      "Epoch 21, Gen Loss: 0.7430744171142578, Disc Loss: 1.336962342262268\n",
      "Epoch 22, Gen Loss: 0.8107817769050598, Disc Loss: 1.314685344696045\n",
      "Epoch 23, Gen Loss: 0.7222557067871094, Disc Loss: 1.4065546989440918\n",
      "Epoch 24, Gen Loss: 0.7781813740730286, Disc Loss: 1.3040351867675781\n",
      "Epoch 25, Gen Loss: 0.745223879814148, Disc Loss: 1.331608772277832\n",
      "Epoch 26, Gen Loss: 0.7630731463432312, Disc Loss: 1.3170061111450195\n",
      "Epoch 27, Gen Loss: 0.7639352083206177, Disc Loss: 1.327566146850586\n",
      "Epoch 28, Gen Loss: 0.7461874485015869, Disc Loss: 1.3248834609985352\n",
      "Epoch 29, Gen Loss: 0.7428171634674072, Disc Loss: 1.3380141258239746\n",
      "Epoch 30, Gen Loss: 0.7138580679893494, Disc Loss: 1.3595356941223145\n",
      "Epoch 31, Gen Loss: 0.7337199449539185, Disc Loss: 1.36448073387146\n",
      "Epoch 32, Gen Loss: 0.7851560115814209, Disc Loss: 1.3160638809204102\n",
      "Epoch 33, Gen Loss: 0.7323093414306641, Disc Loss: 1.350412130355835\n",
      "Epoch 34, Gen Loss: 0.7857284545898438, Disc Loss: 1.3338840007781982\n",
      "Epoch 35, Gen Loss: 0.7448024749755859, Disc Loss: 1.3660297393798828\n",
      "Epoch 36, Gen Loss: 0.7773570418357849, Disc Loss: 1.3320964574813843\n",
      "Epoch 37, Gen Loss: 0.7350282669067383, Disc Loss: 1.3649535179138184\n",
      "Epoch 38, Gen Loss: 0.7513965368270874, Disc Loss: 1.3230717182159424\n",
      "Epoch 39, Gen Loss: 0.7504273653030396, Disc Loss: 1.3408377170562744\n",
      "Epoch 40, Gen Loss: 0.7748457193374634, Disc Loss: 1.3422714471817017\n",
      "Epoch 41, Gen Loss: 0.7547355890274048, Disc Loss: 1.3486427068710327\n",
      "Epoch 42, Gen Loss: 0.7690541744232178, Disc Loss: 1.3261860609054565\n",
      "Epoch 43, Gen Loss: 0.7642292976379395, Disc Loss: 1.3422083854675293\n",
      "Epoch 44, Gen Loss: 0.7220210433006287, Disc Loss: 1.3472094535827637\n",
      "Epoch 45, Gen Loss: 0.764556348323822, Disc Loss: 1.325317621231079\n",
      "Epoch 46, Gen Loss: 0.7661231160163879, Disc Loss: 1.3504297733306885\n",
      "Epoch 47, Gen Loss: 0.7595701813697815, Disc Loss: 1.3210537433624268\n",
      "Epoch 48, Gen Loss: 0.7523130178451538, Disc Loss: 1.3290985822677612\n",
      "Epoch 49, Gen Loss: 0.7523385882377625, Disc Loss: 1.361229658126831\n",
      "Epoch 50, Gen Loss: 0.7519180178642273, Disc Loss: 1.3327927589416504\n",
      "Epoch 51, Gen Loss: 0.7569377422332764, Disc Loss: 1.3111572265625\n",
      "Epoch 52, Gen Loss: 0.7737863659858704, Disc Loss: 1.3314927816390991\n",
      "Epoch 53, Gen Loss: 0.7378696203231812, Disc Loss: 1.355211615562439\n",
      "Epoch 54, Gen Loss: 0.7835866212844849, Disc Loss: 1.309931755065918\n",
      "Epoch 55, Gen Loss: 0.731131374835968, Disc Loss: 1.36653470993042\n",
      "Epoch 56, Gen Loss: 0.7885975241661072, Disc Loss: 1.3170833587646484\n",
      "Epoch 57, Gen Loss: 0.7535582184791565, Disc Loss: 1.3403894901275635\n",
      "Epoch 58, Gen Loss: 0.7805687189102173, Disc Loss: 1.3177000284194946\n",
      "Epoch 59, Gen Loss: 0.793951690196991, Disc Loss: 1.3119513988494873\n",
      "Epoch 60, Gen Loss: 0.7684873342514038, Disc Loss: 1.3395211696624756\n",
      "Epoch 61, Gen Loss: 0.7573102116584778, Disc Loss: 1.3388113975524902\n",
      "Epoch 62, Gen Loss: 0.7591779232025146, Disc Loss: 1.3635594844818115\n",
      "Epoch 63, Gen Loss: 0.7621156573295593, Disc Loss: 1.3696502447128296\n",
      "Epoch 64, Gen Loss: 0.7689244747161865, Disc Loss: 1.3142000436782837\n",
      "Epoch 65, Gen Loss: 0.7844333648681641, Disc Loss: 1.3120431900024414\n",
      "Epoch 66, Gen Loss: 0.7867846488952637, Disc Loss: 1.3236970901489258\n",
      "Epoch 67, Gen Loss: 0.775394082069397, Disc Loss: 1.3273680210113525\n",
      "Epoch 68, Gen Loss: 0.772975504398346, Disc Loss: 1.3111776113510132\n",
      "Epoch 69, Gen Loss: 0.7727816700935364, Disc Loss: 1.3579318523406982\n",
      "Epoch 70, Gen Loss: 0.7684099674224854, Disc Loss: 1.348417043685913\n",
      "Epoch 71, Gen Loss: 0.776222825050354, Disc Loss: 1.3424875736236572\n",
      "Epoch 72, Gen Loss: 0.7642959356307983, Disc Loss: 1.3374335765838623\n",
      "Epoch 73, Gen Loss: 0.748896062374115, Disc Loss: 1.311694860458374\n",
      "Epoch 74, Gen Loss: 0.7701865434646606, Disc Loss: 1.318737506866455\n",
      "Epoch 75, Gen Loss: 0.77348792552948, Disc Loss: 1.2842676639556885\n",
      "Epoch 76, Gen Loss: 0.7596149444580078, Disc Loss: 1.337790846824646\n",
      "Epoch 77, Gen Loss: 0.7458507418632507, Disc Loss: 1.3516969680786133\n",
      "Epoch 78, Gen Loss: 0.7486477494239807, Disc Loss: 1.342186450958252\n",
      "Epoch 79, Gen Loss: 0.7868150472640991, Disc Loss: 1.3219974040985107\n",
      "Epoch 80, Gen Loss: 0.7953864932060242, Disc Loss: 1.3222451210021973\n",
      "Epoch 81, Gen Loss: 0.7289482355117798, Disc Loss: 1.364924430847168\n",
      "Epoch 82, Gen Loss: 0.7783958911895752, Disc Loss: 1.342696189880371\n",
      "Epoch 83, Gen Loss: 0.8048616647720337, Disc Loss: 1.3261891603469849\n",
      "Epoch 84, Gen Loss: 0.7880443334579468, Disc Loss: 1.3202834129333496\n",
      "Epoch 85, Gen Loss: 0.7704071998596191, Disc Loss: 1.3288562297821045\n",
      "Epoch 86, Gen Loss: 0.7646079063415527, Disc Loss: 1.322689175605774\n",
      "Epoch 87, Gen Loss: 0.7514979839324951, Disc Loss: 1.3590091466903687\n",
      "Epoch 88, Gen Loss: 0.7941118478775024, Disc Loss: 1.2988659143447876\n",
      "Epoch 89, Gen Loss: 0.746195912361145, Disc Loss: 1.354844331741333\n",
      "Epoch 90, Gen Loss: 0.8308627605438232, Disc Loss: 1.2786033153533936\n",
      "Epoch 91, Gen Loss: 0.7187312841415405, Disc Loss: 1.3808331489562988\n",
      "Epoch 92, Gen Loss: 0.8207642436027527, Disc Loss: 1.2959794998168945\n",
      "Epoch 93, Gen Loss: 0.7286447286605835, Disc Loss: 1.3914697170257568\n",
      "Epoch 94, Gen Loss: 0.7802125215530396, Disc Loss: 1.3214845657348633\n",
      "Epoch 95, Gen Loss: 0.8135743141174316, Disc Loss: 1.2933681011199951\n",
      "Epoch 96, Gen Loss: 0.7935943603515625, Disc Loss: 1.2949156761169434\n",
      "Epoch 97, Gen Loss: 0.8134769797325134, Disc Loss: 1.325608491897583\n",
      "Epoch 98, Gen Loss: 0.7758432030677795, Disc Loss: 1.3218696117401123\n",
      "Epoch 99, Gen Loss: 0.8310195207595825, Disc Loss: 1.3041446208953857\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = x_scaled.shape[0]\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_scaled, y))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for real_data, real_labels in train_dataset:\n",
    "        g_loss, d_loss = train_step(real_data, real_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Gen Loss: {g_loss.numpy()}, Disc Loss: {d_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(generator, num_samples, noise_dim, label_dim, class_label):\n",
    "    noise = tf.random.normal([num_samples, noise_dim])\n",
    "    labels = np.full((num_samples, label_dim), class_label)\n",
    "    generated_data = generator([noise, labels], training=False)\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 1000 synthetic samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [0, 1]  # Binary labels\n",
    "synthetic_data = []\n",
    "synthetic_labels = []\n",
    "\n",
    "for class_label in class_labels:\n",
    "    synthetic_samples = generate_synthetic_data(\n",
    "        generator, 1000, noise_dim, label_dim, class_label\n",
    "    )\n",
    "    synthetic_data.append(synthetic_samples)\n",
    "    synthetic_labels.extend([class_label] * 1000)\n",
    "\n",
    "synthetic_data = np.vstack(synthetic_data)\n",
    "synthetic_labels = np.array(synthetic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse transform to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = scaler.inverse_transform(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=x.columns)\n",
    "synthetic_df[\"Class\"] = synthetic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V17</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.638477</td>\n",
       "      <td>1.337374</td>\n",
       "      <td>-5.469562</td>\n",
       "      <td>0.017342</td>\n",
       "      <td>-1.322709</td>\n",
       "      <td>-0.913055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.064563</td>\n",
       "      <td>1.893935</td>\n",
       "      <td>4.957232</td>\n",
       "      <td>-0.327365</td>\n",
       "      <td>-0.216767</td>\n",
       "      <td>0.309035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.499691</td>\n",
       "      <td>-0.157214</td>\n",
       "      <td>3.028811</td>\n",
       "      <td>3.475270</td>\n",
       "      <td>-0.382252</td>\n",
       "      <td>-0.964402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.838434</td>\n",
       "      <td>-0.573273</td>\n",
       "      <td>-3.976091</td>\n",
       "      <td>-0.493800</td>\n",
       "      <td>0.118743</td>\n",
       "      <td>-1.246866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.337239</td>\n",
       "      <td>-1.622592</td>\n",
       "      <td>-2.131279</td>\n",
       "      <td>0.611594</td>\n",
       "      <td>0.408417</td>\n",
       "      <td>-2.078907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V4       V10       V11       V12       V14       V17  Class\n",
       "0 -3.638477  1.337374 -5.469562  0.017342 -1.322709 -0.913055      0\n",
       "1 -3.064563  1.893935  4.957232 -0.327365 -0.216767  0.309035      0\n",
       "2  4.499691 -0.157214  3.028811  3.475270 -0.382252 -0.964402      0\n",
       "3 -0.838434 -0.573273 -3.976091 -0.493800  0.118743 -1.246866      0\n",
       "4 -4.337239 -1.622592 -2.131279  0.611594  0.408417 -2.078907      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    1000\n",
       "1    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv(\"datasets/creditcard_synthetic.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
